{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import metrics as mt\n",
    "import settings as opt\n",
    "import weights\n",
    "from preprocess.dataset import ValenceArousalWithClassesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Choose model and dataset\n",
    "\n",
    "# Select dataset to train on - possible choices: affective, augmented_affective, cifar10\n",
    "dataset_choice = \"affective\"\n",
    "\n",
    "# Select model to train on - PAGAN, PAGAN_D, PAGAN_SN\n",
    "model_choice = \"PAGAN\"\n",
    "\n",
    "import model.pagan as gan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gen_imgs_path = opt.outi + model_choice + '/' + dataset_choice + '/'\n",
    "save_model_checkpoints = opt.outc + model_choice + '/' + dataset_choice + '/'\n",
    "\n",
    "# create directories for output\n",
    "try:\n",
    "    os.makedirs(gen_imgs_path)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs(save_model_checkpoints)\n",
    "except OSError:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "pre_process = transforms.Compose([\n",
    "    transforms.Resize(opt.image_size),\n",
    "    transforms.CenterCrop(opt.image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "if dataset_choice == \"cifar10\":\n",
    "    dataset = dset.CIFAR10(\n",
    "        root=\"../data/cifar10\",\n",
    "        download=True,\n",
    "        transform=pre_process\n",
    "    )\n",
    "    n_classes = 10\n",
    "elif dataset_choice == \"affective\":\n",
    "    dataset = ValenceArousalWithClassesDataset(csv_file=opt.annonations_file,\n",
    "                                               root_dir=opt.all_images_path,\n",
    "                                               transform=pre_process)\n",
    "    n_classes = 13\n",
    "else:\n",
    "    dataset = ValenceArousalWithClassesDataset(csv_file=opt.augmented_annonations_file,\n",
    "                                               root_dir=opt.augmented_images_path,\n",
    "                                               transform=pre_process)\n",
    "    n_classes = 13\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=opt.batch_size, shuffle=True, num_workers=4,\n",
    "                        drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the GAN model\n",
    "\n",
    "# Create or load the generator\n",
    "generator = gan.Generator(opt.latent_dim, opt.num_channels, opt.ngf, opt.n_classes).to(device)\n",
    "generator.apply(weights.weights_init)\n",
    "\n",
    "# Create or load the discriminator\n",
    "if model_choice == \"PAGAN\":\n",
    "    discriminator = gan.Discriminator(opt.num_channels, opt.ndf, opt.n_classes).to(device)\n",
    "elif model_choice == \"PAGAN_D\":\n",
    "    discriminator = gan.DiscriminatorDropout(opt.num_channels, opt.ndf, opt.n_classes).to(device)\n",
    "else:\n",
    "    discriminator = gan.DiscriminatorSN(opt.num_channels, opt.ndf, opt.n_classes).to(device)\n",
    "\n",
    "discriminator.apply(weights.weights_init)\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "# with condition GAN, these fields are unnecessary\n",
    "real_label = 1.  # GAN trick, real examples are real in 90%\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=opt.lr_D, betas=(opt.beta1, opt.beta2))\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=opt.lr_G, betas=(opt.beta1, opt.beta2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# PA-GAN HyperParameters\n",
    "\n",
    "# keep actual augmentation level\n",
    "augmentation_level = 0\n",
    "last_augmentation_step = 0\n",
    "\n",
    "# steps for p to reach 0.5\n",
    "tr = 5000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "fid_score_history = []\n",
    "kid_score = []\n",
    "kid_score_history = []\n",
    "\n",
    "fake_images_list = []\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(opt.epochs):\n",
    "\n",
    "    ##############################\n",
    "    ### COMPUTE METRICS\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Global step: {}. Computing metrics...\".format(global_step))\n",
    "\n",
    "        # get random real samples\n",
    "        samples = random.sample(range(len(dataset)), opt.fid_batch)\n",
    "        real_samples = [dataset[s][0] for s in samples]\n",
    "        real_samples = torch.stack(real_samples, dim=0).to(device)\n",
    "\n",
    "        # generate random fake samples\n",
    "        fake_samples = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            noise = Variable(\n",
    "                torch.FloatTensor(np.random.normal(0, 1, (opt.fid_batch, opt.latent_dim)))).to(\n",
    "                device)\n",
    "            gen_labels = Variable(\n",
    "                torch.LongTensor(np.random.randint(0, n_classes, opt.fid_batch))).to(device)\n",
    "\n",
    "            for k in tqdm(range(opt.fid_batch), desc=\"Generating fake images\"):\n",
    "                noise_ = noise[k * opt.batch_size: (k + 1) * opt.batch_size]\n",
    "                gen_labels_ = gen_labels[k * opt.batch_size: (k + 1) * opt.batch_size]\n",
    "\n",
    "                fake_samples.append(generator(noise_, gen_labels_))\n",
    "            fake_samples = torch.cat(fake_samples, dim=0).to(device)\n",
    "\n",
    "        print(\"Computing KID and FID...\")\n",
    "        kid, fid = mt.compute_metrics(real_samples, fake_samples)\n",
    "\n",
    "        print(\"FID: {:.4f}\".format(fid))\n",
    "        print(\"KID: {:.4f}\".format(kid))\n",
    "\n",
    "        fid_score_history.append(fid)\n",
    "        # Augment discriminator dimension\n",
    "        if (len(kid_score) >= 2 and kid >= (\n",
    "                kid_score[-1] + kid_score[-2]) * 19 / 40):\n",
    "            # there should be calculated KID score and depending on the result and previous ones decide whether increment augmentation level\n",
    "            augmentation_level += 1\n",
    "            last_augmentation_step = global_step\n",
    "            discriminator.main.conv1 = nn.Conv2d(opt.num_channels + augmentation_level,\n",
    "                                                 opt.ndf, 4, 2, 1, bias=False).to(\n",
    "                device)\n",
    "            discriminator.num_channels = discriminator.num_channels + augmentation_level\n",
    "            discriminator.main.conv1.apply(weights.weights_init)\n",
    "            optimizerD = optim.Adam(discriminator.parameters(), lr=opt.lr_D,\n",
    "                                    betas=(opt.beta1, opt.beta2))\n",
    "\n",
    "            print(\"Augmentation level increased to {}\".format(augmentation_level))\n",
    "\n",
    "            kid_score = []\n",
    "            kid_score_history.append(kid)\n",
    "        else:\n",
    "            kid_score.append(kid)\n",
    "            kid_score_history.append(kid)\n",
    "\n",
    "    ##############################\n",
    "\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, start=0):\n",
    "\n",
    "        # Format batch\n",
    "        real_images = data[0].to(device)\n",
    "        # real_labels = data[1].to(device)  # Without condition in GAN, this is unnecessary\n",
    "\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        true_labels = Variable(torch.FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False).to(\n",
    "            device).view(-1)\n",
    "        false_labels = Variable(torch.FloatTensor(batch_size, 1).fill_(0.0),\n",
    "                                requires_grad=False).to(device).view(-1)\n",
    "\n",
    "        # Configure input\n",
    "        real_images = Variable(real_images.type(torch.FloatTensor)).to(device)\n",
    "        # real_labels = Variable(real_labels.type(torch.LongTensor)).to(device).view(-1)\n",
    "\n",
    "        # Generate batch of latent vectors\n",
    "        noise = Variable(\n",
    "            torch.FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim)))).to(device)\n",
    "        gen_labels = Variable(torch.LongTensor(np.random.randint(0, n_classes, batch_size))).to(\n",
    "            device)\n",
    "        # Generate fake image batch with G\n",
    "\n",
    "        fake_images = generator(noise, gen_labels)\n",
    "\n",
    "        ###################################\n",
    "        # (1) Update D network\n",
    "        # Train with all-real batch\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        if augmentation_level > 0:\n",
    "            p = min(0.5 * (global_step - last_augmentation_step) / tr, 0.5)\n",
    "            if augmentation_level > 1:\n",
    "                augmentation_bits_old = np.random.randint(0, 2,\n",
    "                                                          size=(batch_size, augmentation_level - 1))\n",
    "                augmentation_bits_new = np.where(np.random.rand(batch_size, 1) < p,\n",
    "                                                 np.ones((batch_size, 1)),\n",
    "                                                 np.zeros((batch_size, 1)))\n",
    "                augmentation_bits = np.concatenate((augmentation_bits_old, augmentation_bits_new),\n",
    "                                                   axis=1)\n",
    "            else:\n",
    "                augmentation_bits = np.where(np.random.rand(batch_size, 1) < p,\n",
    "                                             np.ones((batch_size, 1)),\n",
    "                                             np.zeros((batch_size, 1)))\n",
    "        else:\n",
    "            augmentation_bits = None\n",
    "\n",
    "        real_augmented, real_labels_augmented = gan.add_channel(real_images, augmentation_bits,\n",
    "                                                                real=True)\n",
    "\n",
    "        real_images = real_augmented\n",
    "        label = real_labels_augmented\n",
    "\n",
    "        # Forward pass real batch through D\n",
    "        output = discriminator(real_images).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = adversarial_loss(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        # D_x = output.mean().item()\n",
    "\n",
    "        # Add channel to images\n",
    "        fake_augmented, fake_labels_augmented = gan.add_channel(fake_images, augmentation_bits,\n",
    "                                                                real=False)\n",
    "\n",
    "        fake_images = fake_augmented\n",
    "        fake_label = fake_labels_augmented\n",
    "\n",
    "        # Classify all fake batch with D\n",
    "        output = discriminator(fake_images.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = adversarial_loss(output, fake_label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        # D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ###################################\n",
    "        # (2) Update G network\n",
    "        generator.zero_grad()\n",
    "        # label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = discriminator(fake_images).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "\n",
    "        errG = adversarial_loss(output, 1 - fake_label)\n",
    "\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        # D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        # Output training stats\n",
    "        if i % opt.log_interval == 0:\n",
    "            print(\n",
    "                \"[{}/{}][{}/{}] Loss_D: {:.4f} Loss_G: {:.4f}\".format(\n",
    "                    epoch,\n",
    "                    opt.epochs,\n",
    "                    i,\n",
    "                    len(dataloader),\n",
    "                    errD.item(),\n",
    "                    errG.item()\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Save generated images after each epoch\n",
    "    noise = Variable(torch.FloatTensor(np.random.normal(0, 1, (64, opt.latent_dim)))).to(device)\n",
    "    gen_labels = Variable(torch.LongTensor(np.random.randint(0, n_classes, 64))).to(device)\n",
    "\n",
    "    fake_images = generator(noise, gen_labels).detach().cpu()\n",
    "\n",
    "    vutils.save_image(fake_images,\n",
    "                      \"%s/fake_%s_epoch_%03d.png\" % (gen_imgs_path, dataset_choice, epoch),\n",
    "                      normalize=True)\n",
    "\n",
    "    # Save generator and discriminator weights after each 20 epochs\n",
    "    if epoch % 20 == 0:\n",
    "        torch.save(generator.state_dict(),\n",
    "                   \"%s/netG_%s_epoch_%d.pth\" % (save_model_checkpoints, dataset_choice, epoch))\n",
    "        torch.save(discriminator.state_dict(),\n",
    "                   \"%s/netD_%s_epoch_%d.pth\" % (save_model_checkpoints, dataset_choice, epoch))\n",
    "\n",
    "print(\"Training is finished!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Generator and Discriminator loss during training\")\n",
    "plt.plot(G_losses, label=\"Generator\")\n",
    "plt.plot(D_losses, label=\"Discriminator\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# FID results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"FID score\")\n",
    "plt.plot(fid_score_history)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"FID value\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# KID results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"KID score\")\n",
    "plt.plot(kid_score_history)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"KID value\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(fid_score_history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(kid_score_history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(G_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(D_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}